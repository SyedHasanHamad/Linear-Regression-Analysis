# Q (C) iii. Linear regression with variables (stepwise) selected using AIC
mod3 <- stepAIC(lm(brozek ~ ., data = fat1train), direction = "both", trace = FALSE)
# q (C) ii. 5 best predictors
library(leaps)
a2 <- regsubsets(brozek ~ ., data= fat1train, nvmax=5, nbest= 120,
method= c("exhaustive"), really.big= T);
models2 <- summary(a2)$which;
models2.size <- as.numeric(attr( models2, "dimnames")[[1]]);
models2.rss <- summary(a2)$rss;
op2 <- which( models2.size == 5);
flag2 <- op2[which.min( models2.rss[op2])];
# Manual look at the selected model
models2[flag2,]
model2 <- lm( brozek ~ siri + density + thigh + knee +forearm, data = fat1train)
pred2 <- predict(model2, fat1test[,2:18]);
MSE2mod2 <- mean((pred2 - fat1test[,1])^2);
# q (C) ii. 5 best predictors
library(leaps)
a2 <- regsubsets(brozek ~ ., data= fat1train, nvmax=5, nbest= 120,
method= c("exhaustive"), really.big= T);
models2 <- summary(a2)$which;
models2.size <- as.numeric(attr( models2, "dimnames")[[1]]);
models2.rss <- summary(a2)$rss;
op2 <- which( models2.size == 5);
flag2 <- op2[which.min( models2.rss[op2])];
# Manual look at the selected model
models2[flag2,]
mod2 <- lm( brozek ~ siri + density + thigh + knee +forearm, data = fat1train)
pred2 <- predict(mod2, fat1test[,2:18]);
MSE2mod2 <- mean((pred2 - fat1test[,1])^2);
print(MSE2mod2)
# q (C) ii. 5 best predictors
library(leaps)
a2 <- regsubsets(brozek ~ ., data= fat1train, nvmax=5, nbest= 120,
models2 <- summary(a2)$which;
# q (C) ii. 5 best predictors
library(leaps)
a2 <- regsubsets(brozek ~ ., data= fat1train, nvmax=5, nbest= 120,
method= c("exhaustive"), really.big= T);
models2 <- summary(a2)$which;
models2.size <- as.numeric(attr( models2, "dimnames")[[1]]);
models2.rss <- summary(a2)$rss;
op2 <- which( models2.size == 5);
flag2 <- op2[which.min( models2.rss[op2])];
# Manual look at the selected model
models2[flag2,]
mod2 <- lm( brozek ~ siri + density + thigh + knee +forearm, data = fat1train)
pred2 <- predict(mod2, fat1test[,2:18]);
MSE2mod2 <- mean((pred2 - fat1test[,1])^2);
print(MSE2mod2)
# q (C) ii. 5 best predictors
library(leaps)
a2 <- regsubsets(brozek ~ ., data= fat1train, nvmax=5, nbest= 120,
method= c("exhaustive"), really.big= T);
models2 <- summary(a2)$which;
models2.size <- as.numeric(attr( models2, "dimnames")[[1]]);
models2.rss <- summary(a2)$rss;
op2 <- which( models2.size == 5);
flag2 <- op2[which.min( models2.rss[op2])];
models2[flag2,]
mod2 <- lm( brozek ~ siri + density + thigh + knee +forearm, data = fat1train)
pred2 <- predict(mod2, fat1test[,2:18]);
MSE2mod2 <- mean((pred2 - fat1test[,1])^2);
print(MSE2mod2)
# Q (C) i. Linear Regression with all predictors
mod1 <- lm(brozek ~ ., data = fat1train)
# Linear Regression with all predictors
pred1.test <- predict.lm(mod1, newdata = fat1test)
error1.test <- mean((pred1.test - fat1test$brozek)^2)
# Q (C) iii. Linear regression with variables (stepwise) selected using
model3 <- step(a1)
# Q (C) iii. Linear regression with variables (stepwise) selected using
model3 <- step(mod1)
round(coef(model3),3)
summary(model3)
pred3 <- predict(model3, fat1test[,2:18]);
MSE2mod3 <- mean((pred3 - fat1test[,1])^2);
# Q (C) iii. Linear regression with variables (stepwise) selected using
model3 <- step(mod1)
round(coef(model3),3)
pred3 <- predict(model3, fat1test[,2:18]);
MSE2mod3 <- mean((pred3 - fat1test[,1])^2);
# q (C) iv. Ridge regression
library(glmnet)
fat.ridge <- lm.ridge( brozek ~ ., data = fat1train, lambda= seq(0,100,0.001))
# q (C) iv. Ridge regression
library(glmnet)
fat.ridge <- lm.ridge( brozek ~ ., data = fat1train, lambda= seq(0,100,0.001))
lambdaopt <- which.min(fat.ridge$GCV);
# q (C) iv. Ridge regression
library(glmnet)
fat.ridge <- lm.ridge( brozek ~ ., data = fat1train, lambda= seq(0,100,0.001))
lambdaopt <- which.min(fat.ridge$GCV);
rig1coef <- fat.ridge$coef[,lambdaopt];
## find the intercepts using ybar and xbar from training data
rig1intercepts <- fat.ridge$ym - sum(fat.ridge$xm *(rig1coef / fat.ridge$scales));
# q (C) iv. Ridge regression
library(glmnet)
fat.ridge <- lm.ridge( brozek ~ ., data = fat1train, lambda= seq(0,100,0.001))
lambdaopt <- which.min(fat.ridge$GCV);
rig1coef <- fat.ridge$coef[,lambdaopt];
## find the intercepts using ybar and xbar from training data
rig1intercepts <- fat.ridge$ym - sum(fat.ridge$xm *(rig1coef / fat.ridge$scales));
pred4 <- scale(fat1test[,2:18], center = F, scale = fat.ridge$scales)%*%
rig1coef + rig1intercepts;
MSE1mod4 <- mean( (pred4 - fat1test[,1])^2);
MSE1mod4
# q (C) v. LASSO regression
library(lars)
install.packages("lars")
# q (C) v. LASSO regression
library(lars)
fat.lars <- lars( as.matrix(fat1train[,2:18]),
fat1train[,1], type= "lasso", trace= TRUE);
# q (C) v. LASSO regression
library(lars)
fat.lars <- lars( as.matrix(fat1train[,2:18]),
# q (C) v. LASSO regression
library(lars)
fat.lars <- lars( as.matrix(fat1train[,2:18])
# q (C) v. LASSO regression
library(lars)
fat.lars <- lars( as.matrix(fat1train[,2:18]),
fat1train[,1], type= "lasso", trace= TRUE);
# q (C) v. LASSO regression
library(lars)
fat.lars <- lars( as.matrix(fat1train[,2:18]),
fat1train[,1], type= "lasso", trace= TRUE);
Cp1 <- summary(fat.lars)$Cp;
index1 <- which.min(Cp1);
lasso.lambda <- fat.lars$lambda[index1];
# q (C) v. LASSO regression
library(lars)
fat.lars <- lars( as.matrix(fat1train[,2:18]),
fat1train[,1], type= "lasso", trace= TRUE);
Cp1 <- summary(fat.lars)$Cp;
index1 <- which.min(Cp1);
lasso.lambda <- fat.lars$lambda[index1];
fit5b <- predict(fat.lars, fat1test[,-1], s=lasso.lambda, type="fit", mode="lambda");
yhat5b <- fit5b$fit;
MSE2mod5 <- mean( (yhat5b - fat1test[,1])^2);
# q (C) vi. Princippal Component regression
# Principal Component Regression (PCR)
library(caret)
library(pls)
# Scale the training and testing data
x_train_scaled <- scale(fat1train[, -1])
x_test_scaled <- scale(fat1test[, -1])
# Perform PCA on the training data
pc <- prcomp(x_train_scaled, center = TRUE, scale. = TRUE)
# Select the number of components (e.g., 5 components)
num_components <- 5
x_train_pcr <- pc$x[, 1:num_components]
# Build a PCR model on the training data
pcr_model <- lm(brozek ~ x_train_pcr, data = fat1train)
# Make predictions on the testing data
x_test_pcr <- predict(pc, newdata = data.frame(x_test = x_test_scaled))[, 1:num_components]
# Principal Component Regression (PCR)
library(caret)
library(pls)
# Scale the training and testing data
x_train_scaled <- scale(fat1train[, -1])
x_test_scaled <- scale(fat1test[, -1])
# Perform PCA on the training data
pc <- prcomp(x_train_scaled, center = TRUE, scale. = TRUE)
# Select the number of components (e.g., 5 components)
num_components <- 5
x_train_pcr <- pc$x[, 1:num_components]
# Build a PCR model on the training data
pcr_model <- lm(brozek ~ x_train_pcr, data = fat1train)
# Transform the testing data using the same PCA transformation
x_test_pcr <- predict(pc, newdata = as.data.frame(x_test_scaled))[, 1:num_components]
# Make predictions on the testing data
y_pred_pcr <- predict(pcr_model, newdata = data.frame(x_train_pcr = x_test_pcr))
# Calculate the testing error for PCR
testing_error_pcr <- mean((fat1test$brozek - y_pred_pcr)^2)
# Print the testing error
cat("Testing Error for Principal Component Regression (PCR):", testing_error_pcr, "\n")
# Principal Component Regression (PCR)
library(caret)
library(pls)
# Scale the training and testing data
x_train_scaled <- scale(fat1train[, -1])
x_test_scaled <- scale(fat1test[, -1])
# Perform PCA on the training data
pc <- prcomp(x_train_scaled, center = TRUE, scale. = TRUE)
# Select the number of components (e.g., 5 components)
num_components <- 5
x_train_pcr <- pc$x[, 1:num_components]
# Build a PCR model on the training data
pcr_model <- lm(brozek ~ x_train_pcr, data = fat1train)
# Transform the testing data using the same PCA transformation
x_test_pcr <- predict(pc, newdata = as.data.frame(x_test_scaled))[, 1:num_components]
# Make predictions on the testing data
y_pred_pcr <- predict(pcr_model, newdata = data.frame(x_train_pcr = x_test_pcr))
# Calculate the testing error for PCR
testing_error_pcr <- mean((fat1test$brozek - y_pred_pcr)^2)
# Print the testing error
cat("Testing Error for Principal Component Regression (PCR):", testing_error_pcr, "\n")
trainpca <- prcomp(fat1train[,2:18]);
trainpca$sdev
round(trainpca$sdev,2)
## Choose a number beyond which all e. values are relatively small
plot(trainpca$sdev,type="l", ylab="SD of PC", xlab="PC number")
## For instance, we want to do Regression on the first 15 PCs
## Get Pcs from obj$x
modelpca <- lm( brozek ~ trainpca$x[,1:17], data = fat1train)
## The PCA on \beta for the original model Y=X\beta + epsilon
## since Y= Z\gamma = (X U) \gamma = X(U\gamma)
beta.pca <- trainpca$rot[,1:17] %*% modelpca$coef[-1];
##as a comparion of \beta for PCA
# Now we use package for pcr
# install.packages(’pls’)
library(pls)
fat.pca <- pcr(brozek ~., data=fat1train, validation="CV");
validationplot(fat.pca);
summary(fat.pca);
10
### Select the optimal number of components
ncompopt <-which.min(fat.pca$validation$adj)
### Cross-Validation shows that 17 PC is the best, but
### please note that your answers might be different due to
### the randomalization of CV.
ncompopt <- 17;
xmean <- apply(fat1train[,2:18], 2, mean);
xtesttransform <- as.matrix(sweep(fat1test[,2:18], 2, xmean));
## New test X on the four PCs
xtestPC <- xtesttransform %*% trainpca$rot[,1:17];
## Predicted Y
ypred6 <- cbind(1, xtestPC) %*% modelpca$coef;
#### b. auto-use "predict()" function directly
ypred6b <- predict(fat.pca, ncomp = ncompopt, newdata = fat1test[2:18]);
## compare these two methods, which should get same results
cbind(ypred6b, ypred6)
### test error for PCA
MSE2mod6 <- mean( (ypred6b - fat1test[,1])^2);
#0.008755981 #based on ncompopt=17 component
trainpca <- prcomp(fat1train[,2:18]);
trainpca$sdev
round(trainpca$sdev,2)
## Choose a number beyond which all e. values are relatively small
plot(trainpca$sdev,type="l", ylab="SD of PC", xlab="PC number")
## For instance, we want to do Regression on the first 15 PCs
## Get Pcs from obj$x
modelpca <- lm( brozek ~ trainpca$x[,1:17], data = fat1train)
## The PCA on \beta for the original model Y=X\beta + epsilon
## since Y= Z\gamma = (X U) \gamma = X(U\gamma)
beta.pca <- trainpca$rot[,1:17] %*% modelpca$coef[-1];
##as a comparion of \beta for PCA
# Now we use package for pcr
# install.packages(’pls’)
library(pls)
fat.pca <- pcr(brozek ~., data=fat1train, validation="CV");
validationplot(fat.pca);
summary(fat.pca);
10
### Select the optimal number of components
ncompopt <-which.min(fat.pca$validation$adj)
### Cross-Validation shows that 17 PC is the best, but
### please note that your answers might be different due to
### the randomalization of CV.
ncompopt <- 17;
xmean <- apply(fat1train[,2:18], 2, mean);
xtesttransform <- as.matrix(sweep(fat1test[,2:18], 2, xmean));
## New test X on the four PCs
xtestPC <- xtesttransform %*% trainpca$rot[,1:17];
## Predicted Y
ypred6 <- cbind(1, xtestPC) %*% modelpca$coef;
#### b. auto-use "predict()" function directly
ypred6b <- predict(fat.pca, ncomp = ncompopt, newdata = fat1test[2:18]);
## compare these two methods, which should get same results
cbind(ypred6b, ypred6)
### test error for PCA
MSE2mod6 <- mean( (ypred6b - fat1test[,1])^2);
MSE2mod6
# Principal Component Analysis (PCA) on the training data
trainpca <- prcomp(fat1train[, 2:18])
trainpca$sdev
round(trainpca$sdev, 2)
# Choose a number beyond which all eigenvalues are relatively small
plot(trainpca$sdev, type = "l", ylab = "SD of PC", xlab = "PC number")
# Regression on the first 17 PCs
modelpca <- lm(brozek ~ trainpca$x[, 1:17], data = fat1train)
# PCA on beta for the original model Y = X*beta + epsilon
beta.pca <- trainpca$rot[, 1:17] %*% modelpca$coef[-1]
# Use the 'pls' package for PCR
library(pls)
fat.pca <- pcr(brozek ~ ., data = fat1train, validation = "CV")
validationplot(fat.pca)
summary(fat.pca)
# Select the optimal number of components
ncompopt <- which.min(fat.pca$validation$adj)
# Standardize the test data and apply PCA
xmean <- apply(fat1train[, 2:18], 2, mean)
xtesttransform <- as.matrix(sweep(fat1test[, 2:18], 2, xmean))
xtestPC <- xtesttransform %*% trainpca$rot[, 1:17]
# Predict Y using the PCR model
ypred6 <- cbind(1, xtestPC) %*% modelpca$coef
# Use the 'predict()' function to predict Y
ypred6b <- predict(fat.pca, ncomp = ncompopt, newdata = fat1test[, 2:18])
# Compare the results from two methods
cbind(ypred6b, ypred6)
# Calculate the testing error for PCR
MSE2mod6 <- mean((ypred6b - fat1test[, 1])^2)
# Q (C) iii. Linear regression with variables (stepwise) selected using
model3 <- step(mod1)
pred3 <- predict(model3, fat1test[,2:18]);
MSE2mod3 <- mean((pred3 - fat1test[,1])^2);
MSE2mod3
# Principal Component Analysis (PCA) on the training data
trainpca <- prcomp(fat1train[, 2:18])
trainpca$sdev
round(trainpca$sdev, 2)
# Choose a number beyond which all eigenvalues are relatively small
plot(trainpca$sdev, type = "l", ylab = "SD of PC", xlab = "PC number")
# Regression on the first 17 PCs
modelpca <- lm(brozek ~ trainpca$x[, 1:17], data = fat1train)
# PCA on beta for the original model Y = X*beta + epsilon
beta.pca <- trainpca$rot[, 1:17] %*% modelpca$coef[-1]
# Use the 'pls' package for PCR
library(pls)
fat.pca <- pcr(brozek ~ ., data = fat1train, validation = "CV")
validationplot(fat.pca)
summary(fat.pca)
# Select the optimal number of components
ncompopt <- which.min(fat.pca$validation$adj)
# Standardize the test data and apply PCA
xmean <- apply(fat1train[, 2:18], 2, mean)
xtesttransform <- as.matrix(sweep(fat1test[, 2:18], 2, xmean))
xtestPC <- xtesttransform %*% trainpca$rot[, 1:17]
# Predict Y using the PCR model
ypred6 <- cbind(1, xtestPC) %*% modelpca$coef
# Use the 'predict()' function to predict Y
ypred6b <- predict(fat.pca, ncomp = ncompopt, newdata = fat1test[, 2:18])
# Compare the results from two methods
cbind(ypred6b, ypred6)
# Calculate the testing error for PCR
MSE2mod6 <- mean((ypred6b - fat1test[, 1])^2)
# Principal Component Analysis (PCA) on the training data
trainpca <- prcomp(fat1train[, 2:18])
trainpca$sdev
round(trainpca$sdev, 2)
# Regression on the first 17 PCs
modelpca <- lm(brozek ~ trainpca$x[, 1:17], data = fat1train)
# PCA on beta for the original model Y = X*beta + epsilon
beta.pca <- trainpca$rot[, 1:17] %*% modelpca$coef[-1]
# Use the 'pls' package for PCR
library(pls)
fat.pca <- pcr(brozek ~ ., data = fat1train, validation = "CV")
validationplot(fat.pca)
summary(fat.pca)
# Select the optimal number of components
ncompopt <- which.min(fat.pca$validation$adj)
# Standardize the test data and apply PCA
xmean <- apply(fat1train[, 2:18], 2, mean)
xtesttransform <- as.matrix(sweep(fat1test[, 2:18], 2, xmean))
xtestPC <- xtesttransform %*% trainpca$rot[, 1:17]
# Predict Y using the PCR model
ypred6 <- cbind(1, xtestPC) %*% modelpca$coef
# Use the 'predict()' function to predict Y
ypred6b <- predict(fat.pca, ncomp = ncompopt, newdata = fat1test[, 2:18])
# Compare the results from two methods
cbind(ypred6b, ypred6)
# Calculate the testing error for PCR
MSE2mod6 <- mean((ypred6b - fat1test[, 1])^2)
# Principal Component Analysis (PCA) on the training data
trainpca <- prcomp(fat1train[, 2:18])
trainpca$sdev
round(trainpca$sdev, 2)
# Regression on the first 17 PCs
modelpca <- lm(brozek ~ trainpca$x[, 1:17], data = fat1train)
# PCA on beta for the original model Y = X*beta + epsilon
beta.pca <- trainpca$rot[, 1:17] %*% modelpca$coef[-1]
# Use the 'pls' package for PCR
library(pls)
fat.pca <- pcr(brozek ~ ., data = fat1train, validation = "CV")
validationplot(fat.pca)
summary(fat.pca)
# Select the optimal number of components
ncompopt <- which.min(fat.pca$validation$adj)
# Standardize the test data and apply PCA
xmean <- apply(fat1train[, 2:18], 2, mean)
xtesttransform <- as.matrix(sweep(fat1test[, 2:18], 2, xmean))
xtestPC <- xtesttransform %*% trainpca$rot[, 1:17]
# Predict Y using the PCR model
ypred6 <- cbind(1, xtestPC) %*% modelpca$coef
# Use the 'predict()' function to predict Y
ypred6b <- predict(fat.pca, ncomp = ncompopt, newdata = fat1test[, 2:18])
# Calculate the testing error for PCR
MSE2mod6 <- mean((ypred6b - fat1test[, 1])^2)
# Principal Component Analysis (PCA) on the training data
trainpca <- prcomp(fat1train[, 2:18])
trainpca$sdev
round(trainpca$sdev, 2)
# Regression on the first 17 PCs
modelpca <- lm(brozek ~ trainpca$x[, 1:17], data = fat1train)
# PCA on beta for the original model Y = X*beta + epsilon
beta.pca <- trainpca$rot[, 1:17] %*% modelpca$coef[-1]
# Use the 'pls' package for PCR
library(pls)
fat.pca <- pcr(brozek ~ ., data = fat1train, validation = "CV")
validationplot(fat.pca)
summary(fat.pca)
# Select the optimal number of components
ncompopt <- which.min(fat.pca$validation$adj)
# Standardize the test data and apply PCA
xmean <- apply(fat1train[, 2:18], 2, mean)
xtesttransform <- as.matrix(sweep(fat1test[, 2:18], 2, xmean))
xtestPC <- xtesttransform %*% trainpca$rot[, 1:17]
# Predict Y using the PCR model
ypred6 <- cbind(1, xtestPC) %*% modelpca$coef
# Use the 'predict()' function to predict Y
ypred6b <- predict(fat.pca, ncomp = ncompopt, newdata = fat1test[, 2:18])
# Calculate the testing error for PCR
MSE2mod6 <- mean((ypred6b - fat1test[, 1])^2)
MSE2mod6
# Principal Component Analysis (PCA) on the training data
trainpca <- prcomp(fat1train[, 2:18])
trainpca$sdev
round(trainpca$sdev, 2)
# Regression on the first 17 PCs
modelpca <- lm(brozek ~ trainpca$x[, 1:17], data = fat1train)
# PCA on beta for the original model Y = X*beta + epsilon
beta.pca <- trainpca$rot[, 1:17] %*% modelpca$coef[-1]
# Use the 'pls' package for PCR
library(pls)
fat.pca <- pcr(brozek ~ ., data = fat1train, validation = "CV")
validationplot(fat.pca)
# Select the optimal number of components
ncompopt <- which.min(fat.pca$validation$adj)
# Standardize the test data and apply PCA
xmean <- apply(fat1train[, 2:18], 2, mean)
xtesttransform <- as.matrix(sweep(fat1test[, 2:18], 2, xmean))
xtestPC <- xtesttransform %*% trainpca$rot[, 1:17]
# Predict Y using the PCR model
ypred6 <- cbind(1, xtestPC) %*% modelpca$coef
# Use the 'predict()' function to predict Y
ypred6b <- predict(fat.pca, ncomp = ncompopt, newdata = fat1test[, 2:18])
# Calculate the testing error for PCR
MSE2mod6 <- mean((ypred6b - fat1test[, 1])^2)
MSE2mod6
# Principal Component Analysis (PCA) on the training data
trainpca <- prcomp(fat1train[, 2:18])
trainpca$sdev
round(trainpca$sdev, 2)
# Regression on the first 17 PCs
modelpca <- lm(brozek ~ trainpca$x[, 1:17], data = fat1train)
# PCA on beta for the original model Y = X*beta + epsilon
beta.pca <- trainpca$rot[, 1:17] %*% modelpca$coef[-1]
# Use the 'pls' package for PCR
library(pls)
fat.pca <- pcr(brozek ~ ., data = fat1train, validation = "CV")
# Select the optimal number of components
ncompopt <- which.min(fat.pca$validation$adj)
# Standardize the test data and apply PCA
xmean <- apply(fat1train[, 2:18], 2, mean)
xtesttransform <- as.matrix(sweep(fat1test[, 2:18], 2, xmean))
xtestPC <- xtesttransform %*% trainpca$rot[, 1:17]
# Predict Y using the PCR model
ypred6 <- cbind(1, xtestPC) %*% modelpca$coef
# Use the 'predict()' function to predict Y
ypred6b <- predict(fat.pca, ncomp = ncompopt, newdata = fat1test[, 2:18])
# Calculate the testing error for PCR
MSE2mod6 <- mean((ypred6b - fat1test[, 1])^2)
MSE2mod6
# q (C) vii. Partial least squares.
library(pls)
x_train <- fat1train[, -1]
y_train <- fat1train[, 1]
pls_model <- plsr(y_train ~ ., data = data.frame(y_train, x_train), scale = TRUE, validation = "CV")
# q (C) vii. Partial least squares.
library(pls)
x_train <- fat1train[, -1]
y_train <- fat1train[, 1]
pls_model <- plsr(y_train ~ ., data = data.frame(y_train, x_train), scale = TRUE, validation = "CV")
# Partial least squares
pls_pred <- predict(pls_model, ncomp = 5, newdata = data.frame(x_test), type = "response")
error7.test <- mean((pls_pred - y_test)^2)
gc()
library(faraway)
library(MASS)
data(fat)
n <- dim(fat)[1]
n1 <- round(n / 10)
flag <- c(1, 21, 22, 57, 70, 88, 91, 94, 121, 127, 149, 151, 159, 162,
164, 177, 179, 194, 206, 214, 215, 221, 240, 241, 243)
fat1train <- fat[-flag, ]
fat1test <- fat[flag, ]
dim(fat1train)
hist(fat1train$brozek, main = "Histogram of Brozek's Equation", xlab = "Brozek's Equation")
summary(fat1train)
cor_matrix <- cor(fat1train[, -1])
round(cor_matrix, 2)
# Q (C) i. Linear Regression with all predictors
mod1 <- lm(brozek ~ ., data = fat1train)
# Linear Regression with all predictors
pred1.test <- predict.lm(mod1, newdata = fat1test)
error1.test <- mean((pred1.test - fat1test$brozek)^2)
# q (C) ii. 5 best predictors
library(leaps)
a2 <- regsubsets(brozek ~ ., data= fat1train, nvmax=5, nbest= 120,
method= c("exhaustive"), really.big= T);
models2 <- summary(a2)$which;
models2.size <- as.numeric(attr( models2, "dimnames")[[1]]);
models2.rss <- summary(a2)$rss;
op2 <- which( models2.size == 5);
flag2 <- op2[which.min( models2.rss[op2])];
models2[flag2,]
mod2 <- lm( brozek ~ siri + density + thigh + knee +forearm, data = fat1train)
pred2 <- predict(mod2, fat1test[,2:18]);
MSE2mod2 <- mean((pred2 - fat1test[,1])^2);
print(MSE2mod2)
